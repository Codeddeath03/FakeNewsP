{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load dataset\n",
    "file_path = 'archive/bharatfakenewskosh .csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# âœ… Keep only useful fields\n",
    "df = df[['Statement', 'Eng_Trans_Statement', 'News Body', 'Eng_Trans_News_Body', \n",
    "         'Language', 'Region', 'Platform', 'Text', 'Video', 'Image', 'Label']]\n",
    "\n",
    "# âœ… Fill missing values\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "# âœ… Merge original and translated fields\n",
    "def merge_text(row):\n",
    "    statement = row['Eng_Trans_Statement'] if row['Eng_Trans_Statement'] else row['Statement']\n",
    "    news_body = row['Eng_Trans_News_Body'] if row['Eng_Trans_News_Body'] else row['News Body']\n",
    "    return f\"{statement} {news_body}\"\n",
    "\n",
    "df['combined_text'] = df.apply(merge_text, axis=1)\n",
    "\n",
    "# âœ… Clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    return text.strip()\n",
    "\n",
    "df['cleaned_text'] = df['combined_text'].apply(clean_text)\n",
    "\n",
    "# âœ… Encode binary fields\n",
    "binary_map = {'yes': 1, 'no': 0}\n",
    "df['Text'] = df['Text'].map(binary_map)\n",
    "df['Video'] = df['Video'].map(binary_map)\n",
    "df['Image'] = df['Image'].map(binary_map)\n",
    "\n",
    "# âœ… One-hot encode categorical fields\n",
    "df = pd.get_dummies(df, columns=['Language', 'Region', 'Platform'])\n",
    "\n",
    "# âœ… Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['Label'] = label_encoder.fit_transform(df['Label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\nisha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize text\n",
    "tokens = tokenizer(\n",
    "    list(df['cleaned_text']),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = tokens['input_ids']\n",
    "attention_mask = tokens['attention_mask']\n",
    "labels = torch.tensor(df['Label'].values)\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "train_size = int(0.6 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=6, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=6)\n",
    "test_loader = DataLoader(test_dataset, batch_size=6)\n",
    "\n",
    "# âœ… Load BERT model\n",
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "bert_model = bert_model.to(device)\n",
    "\n",
    "# âœ… Define optimizer and scheduler (1-cycle policy)\n",
    "optimizer = torch.optim.AdamW(bert_model.parameters(), lr=2e-5)\n",
    "num_training_steps = len(train_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# âœ… Train for one epoch\n",
    "bert_model.train()\n",
    "for epoch in range(1):\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        outputs = bert_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "# âœ… Save embeddings for LightGBM\n",
    "bert_model.eval()\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        outputs = bert_model.bert(input_ids, attention_mask=attention_mask)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).cpu().numpy())\n",
    "\n",
    "X_text = np.concatenate(embeddings, axis=0)\n",
    "np.save('fin_bert_embeddings.npy', X_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_metadata = df[['Text', 'Video', 'Image'] + \n",
    "               [col for col in df.columns if col.startswith(('Language_', 'Region_', 'Platform_'))]].values\n",
    "\n",
    "# âœ… Load saved embeddings\n",
    "X_text = np.load('bert_embeddings.npy')\n",
    "\n",
    "# âœ… Combine embeddings with metadata\n",
    "X = np.concatenate((X_text, X_metadata), axis=1)\n",
    "y = df['Label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text = X_text.reshape((X_text.shape[0], 1, X_text.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_layer = Input(shape=(X_text.shape[1], X_text.shape[2]))  # (1, hidden_dim)\n",
    "x = Bidirectional(LSTM(100, return_sequences=False))(input_layer)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "lstm_model = Model(inputs=input_layer, outputs=output)\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m738/738\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.6021 - loss: 0.6715 - val_accuracy: 0.6162 - val_loss: 0.6667\n",
      "Epoch 2/10\n",
      "\u001b[1m738/738\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6124 - loss: 0.6663 - val_accuracy: 0.6159 - val_loss: 0.6676\n",
      "Epoch 3/10\n",
      "\u001b[1m738/738\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6067 - loss: 0.6682 - val_accuracy: 0.6162 - val_loss: 0.6661\n",
      "Epoch 4/10\n",
      "\u001b[1m738/738\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6085 - loss: 0.6664 - val_accuracy: 0.6166 - val_loss: 0.6690\n",
      "Epoch 5/10\n",
      "\u001b[1m738/738\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6147 - loss: 0.6631 - val_accuracy: 0.6170 - val_loss: 0.6766\n",
      "Epoch 6/10\n",
      "\u001b[1m738/738\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6117 - loss: 0.6597 - val_accuracy: 0.6174 - val_loss: 0.6708\n",
      "Epoch 7/10\n",
      "\u001b[1m738/738\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6180 - loss: 0.6527 - val_accuracy: 0.6136 - val_loss: 0.6677\n",
      "Epoch 8/10\n",
      "\u001b[1m738/738\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6281 - loss: 0.6443 - val_accuracy: 0.6117 - val_loss: 0.6753\n",
      "Epoch 9/10\n",
      "\u001b[1m738/738\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6337 - loss: 0.6358 - val_accuracy: 0.6139 - val_loss: 0.6762\n",
      "Epoch 10/10\n",
      "\u001b[1m738/738\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6495 - loss: 0.6187 - val_accuracy: 0.5682 - val_loss: 0.6972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e196693310>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lstm_model.fit(X_text, y, epochs=10, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data BEFORE tuning\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True),  # âœ… Fixed\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.1, 1.0),         # âœ… Fixed\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        param,\n",
    "        train_data,\n",
    "        num_boost_round=500,  # ğŸ”¥ Increased for better early stopping effect\n",
    "        valid_sets=[train_data, valid_data],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=10),  # âœ… Fixed\n",
    "            lgb.log_evaluation(10)                   # âœ… Cleaner logging\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    y_pred = (model.predict(X_valid) > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Optimize using Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Get best params from tuning\n",
    "best_params = study.best_params\n",
    "print(\"Best Params:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.662751\tvalid_1's binary_logloss: 0.672489\n",
      "[20]\ttraining's binary_logloss: 0.656408\tvalid_1's binary_logloss: 0.672198\n",
      "[30]\ttraining's binary_logloss: 0.650175\tvalid_1's binary_logloss: 0.671885\n",
      "[40]\ttraining's binary_logloss: 0.644094\tvalid_1's binary_logloss: 0.671781\n",
      "[50]\ttraining's binary_logloss: 0.638072\tvalid_1's binary_logloss: 0.671538\n",
      "[60]\ttraining's binary_logloss: 0.63205\tvalid_1's binary_logloss: 0.671294\n",
      "[70]\ttraining's binary_logloss: 0.626272\tvalid_1's binary_logloss: 0.671084\n",
      "[80]\ttraining's binary_logloss: 0.620478\tvalid_1's binary_logloss: 0.670938\n",
      "[90]\ttraining's binary_logloss: 0.614792\tvalid_1's binary_logloss: 0.670705\n",
      "[100]\ttraining's binary_logloss: 0.609309\tvalid_1's binary_logloss: 0.670476\n",
      "[110]\ttraining's binary_logloss: 0.603844\tvalid_1's binary_logloss: 0.670367\n",
      "[120]\ttraining's binary_logloss: 0.598455\tvalid_1's binary_logloss: 0.670311\n",
      "[130]\ttraining's binary_logloss: 0.593235\tvalid_1's binary_logloss: 0.670258\n",
      "[140]\ttraining's binary_logloss: 0.58798\tvalid_1's binary_logloss: 0.670276\n",
      "Early stopping, best iteration is:\n",
      "[135]\ttraining's binary_logloss: 0.590599\tvalid_1's binary_logloss: 0.670199\n",
      "Final LightGBM Accuracy: 0.6010165184243964\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 150,\n",
    "    'learning_rate': 0.0065,\n",
    "    'feature_fraction': 0.1,\n",
    "    'is_unbalance': True\n",
    "}\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "# Add metric explicitly to avoid confusion\n",
    "best_params.update({\n",
    "    'metric': 'binary_logloss',\n",
    "    'verbose': -1,\n",
    "    'is_unbalance': True\n",
    "})\n",
    "# Train final LightGBM model using callbacks\n",
    "final_model = lgb.train(\n",
    "    params=best_params,\n",
    "    train_set=train_data,\n",
    "    num_boost_round=500,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=10),  # Early stopping after 10 rounds\n",
    "        lgb.log_evaluation(period=10)            # Log evaluation every 10 rounds\n",
    "    ]\n",
    ")\n",
    "\n",
    "# âœ… Predict on test set\n",
    "y_pred = (final_model.predict(X_valid) > 0.5).astype(int)\n",
    "\n",
    "# âœ… Evaluate accuracy\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "print(\"Final LightGBM Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttrain's binary_logloss: 0.572132\ttest's binary_logloss: 0.673556\n",
      "Early stopping, best iteration is:\n",
      "[5]\ttrain's binary_logloss: 0.617586\ttest's binary_logloss: 0.671999\n",
      "Final LightGBM Accuracy: 0.6015247776365946\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split embeddings into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_text, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Reshape to 2D for LightGBM\n",
    "X_train_2d = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_2d = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Create LightGBM datasets with labels\n",
    "train_data = lgb.Dataset(X_train_2d, label=y_train)\n",
    "test_data = lgb.Dataset(X_test_2d, label=y_test)\n",
    "# Clean up best_params to avoid conflicts\n",
    "best_params = {k: v for k, v in best_params.items() if k not in ['early_stopping_round', 'verbose']}\n",
    "\n",
    "# Add metric explicitly to avoid confusion\n",
    "best_params.update({\n",
    "    'metric': 'binary_logloss',\n",
    "    'verbose': -1\n",
    "})\n",
    "\n",
    "# Train final LightGBM model using callbacks\n",
    "final_model = lgb.train(\n",
    "    params=best_params,\n",
    "    train_set=train_data,\n",
    "    num_boost_round=500,\n",
    "    valid_sets=[train_data, test_data],\n",
    "    valid_names=['train', 'test'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=10),  # Early stopping after 10 rounds\n",
    "        lgb.log_evaluation(period=10)            # Log evaluation every 10 rounds\n",
    "    ]\n",
    ")\n",
    "\n",
    "# âœ… Predict on test set\n",
    "y_pred = (final_model.predict(X_test_2d) > 0.5).astype(int)\n",
    "\n",
    "# âœ… Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Final LightGBM Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[0;32m      2\u001b[0m best_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_logloss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m }\n\u001b[0;32m     12\u001b[0m final_model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m     13\u001b[0m     best_params,\n\u001b[1;32m---> 14\u001b[0m     \u001b[43mtrain_data\u001b[49m,\n\u001b[0;32m     15\u001b[0m     num_boost_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,  \u001b[38;5;66;03m# More iterations\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     valid_sets\u001b[38;5;241m=\u001b[39m[train_data, test_data],\n\u001b[0;32m     17\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[lgb\u001b[38;5;241m.\u001b[39mearly_stopping(stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m), lgb\u001b[38;5;241m.\u001b[39mlog_evaluation(\u001b[38;5;241m10\u001b[39m)]\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m (final_model\u001b[38;5;241m.\u001b[39mpredict(X_test_2d) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "best_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'learning_rate': 0.09553507428036068,\n",
    "    'num_leaves': 143,\n",
    "    'feature_fraction': 0.4505259438267628,\n",
    "    'max_depth': -1,\n",
    "    'device': 'gpu'\n",
    "}\n",
    "\n",
    "final_model = lgb.train(\n",
    "    best_params,\n",
    "    train_data,\n",
    "    num_boost_round=500,  # More iterations\n",
    "    valid_sets=[train_data, test_data],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(10)]\n",
    ")\n",
    "\n",
    "y_pred = (final_model.predict(X_test_2d) > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Final LightGBM Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
